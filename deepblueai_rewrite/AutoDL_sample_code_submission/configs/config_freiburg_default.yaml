# Codalab submission
extra_packages: []  # Paths
active_model_files: []  # With respect to model_dir
is_submission: False

# Cluster
cluster_datasets_dir: /data/aad/image_datasets/all_symlinks
#cluster_datasets_dir: /data/aad/image_datasets/augmented_datasets
#cluster_datasets_dir: /data/aad/image_datasets/public_datasets
#cluster_datasets_dir: /data/aad/video_datasets/challenge
cluster_model_dir: /home/ferreira/autodl_data/models

# AutoCV, defaults from kakaobrain
autocv:
  model:
    architecture: ResNet18

  optimizer:
    lr: 0.025
    wd: 0.00025
    momentum: 0.9
    type: SGD
    amsgrad: False
    nesterov: True
    scheduler: plateau
    freeze_portion: 0.0
    warmup_multiplier: 2.0
    warm_up_epoch: 5

  dataset:
    train_info_sample: 256
    cv_valid_ratio: 0.1
    max_valid_count: 256
    max_size: 64
    base: 16  # input size should be multipliers of 16
    max_times: 8
    enough_count:
      image: 10000
      video: 1000
    batch_size: 32
    steps_per_epoch: 30
    max_epoch: 1000  # initial value
    batch_size_test: 256

  checkpoints:
    keep: 50

  conditions:
    score_type: auc
    early_epoch: 1
    skip_valid_score_threshold: 0.90  # if bigger then 1.0 is not use
    test_after_at_least_seconds: 1
    test_after_at_least_seconds_max: 90
    test_after_at_least_seconds_step: 2
    threshold_valid_score_diff: 0.001
    threshold_valid_best_score: 0.997
    max_inner_loop_ratio: 0.2
    min_lr: 0.000001  # = 1e-6, need explicit version to be parsed by yaml
    use_fast_auto_aug: True
    output_majority_first: False
    first_simple_model: False
    simple_model: RF
    # In src/winner_cv/skeleton/projects/logic:
    # skip_valid_after_test: min(10 max(3 int(self.info["dataset"]["size"] // 1000)))

autonlp:
  common:
    max_vocab_size: 34513   # maximum number of vocabulary to be embedded, must be larger than number of different words in the dataset
    max_char_length: 153    # maximum number of characters for chinese samples
    max_seq_length: 53      # maximum sequence length for non chinese samples

  model:
    num_epoch: 1              # number of epochs to train the classifier
    total_call_num: 10        # how often the test function shall be called
    valid_ratio: 0.0982       # train/validation split ratio
    increase_batch_acc: 0.669 # batch size will be increased below this accuracy
    early_stop_auc: 0.899     # minimum auc for an early stop
    init_batch_size: 128      # guess what
    ft_dir: ['/app/embedding',
             '/home/ferreira/autodl_data/embedding',
             '/home/dingsda/data/embedding']  # paths to look for the embedding model

  data_manager:
    chi_word_length: 2.72
    max_valid_perclass_sample: 272
    max_sample_train: 13276
    max_train_perclass_sample: 738

  model_manager:
    embedding_dim: 300      # word embedding size

  optimizer:
    lr: 0.00474
    rho: 0.606              # actually the parameter is 1-rho. But this way the config space can be described more easily

autospeech:
  common:
    model_first_max_run_loop: 2
    max_audio_duration: 2           # original comment: limited length of audio, like 20s
    first_round_duration: 14
    middle_duration: 8
    second_round_duration: 57
    audio_sample_rate: 32000
    max_frame_num: 472
    is_cut_audio: False
    num_mfcc: 85                    # original comment: num of mfcc features, default value is 24
    sr: 32000
    fft_duration: 0.179
    hop_duration: 0.0574

  data_manager:
    max_valid_perclass_sample: 287
    min_valid_per_class: 2

  optimizer:
    lr_attention_gru: !!float 0.00276
    lr_bilstm_attention: !!float 0.00179
    lr_cnn: !!float 0.00284
    lr_crnn: !!float 0.00263
    lr_crnn2d: !!float 0.000166
    lr_crnn2d_larger: !!float 0.000108
    lr_crnn2d_vgg: !!float 0.000450
    lr_lstm_attention: 0.000296

    beta_1: 0.0341          # actually the parameter is 1-beta_1. But this way the config space can be described more easily
    beta_2: 0.000125        # actually the parameter is 1-beta_2. But this way the config space can be described more easily
    epsilon: !!float 1.08e-08
    decay: !!float 1.61e-5
    schedule_decay: 0.00302
    amsgrad: False
