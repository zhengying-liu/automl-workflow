# Codalab submission
extra_packages: []  # Paths
active_model_files: []  # With respect to model_dir

# Cluster
cluster_datasets_dir: /data/aad/image_datasets/all_symlinks
#cluster_datasets_dir: /data/aad/image_datasets/public_datasets
#cluster_datasets_dir: /data/aad/video_datasets/challenge
cluster_model_dir: /home/ferreira/autodl_data/models

# AutoCV, defaults from kakaobrain
autocv:
  model:
    architecture: efficientnetb0

  optimizer:
    lr: 0.025

  dataset:
    train_info_sample: 256
    cv_valid_ratio: 0.1
    max_valid_count: 256
    max_size: 64
    base: 16  # input size should be multipliers of 16
    max_times: 8
    enough_count:
      image: 10000
      video: 1000
    batch_size: 32
    steps_per_epoch: 30
    max_epoch: 1000  # initial value
    batch_size_test: 256

  checkpoints:
    keep: 50

  conditions:
    score_type: auc
    early_epoch: 1
    skip_valid_score_threshold: 0.90  # if bigger then 1.0 is not use
    test_after_at_least_seconds: 1
    test_after_at_least_seconds_max: 90
    test_after_at_least_seconds_step: 2
    threshold_valid_score_diff: 0.001
    threshold_valid_best_score: 0.997
    max_inner_loop_ratio: 0.2
    min_lr: 0.000001  # = 1e-6, need explicit version to be parsed by yaml
    use_fast_auto_aug: True
    # In src/winner_cv/skeleton/projects/logic:
    # skip_valid_after_test: min(10 max(3 int(self.info["dataset"]["size"] // 1000)))

autonlp:
  common:
    max_vocab_size: 20000   # maximum number of vocabulary to be embedded, must be larger than number of different words in the dataset
    max_char_length: 96     # maximum number of characters for chinese samples
    max_seq_length: 301     # maximum sequence length for non chinese samples

  model:
    num_epoch: 1              # number of epochs to train the classifier
    valid_ratio: 0.1          # train/validation split ratio
    total_call_num: 20        # how often the test function shall be called
    init_batch_size: 32       # guess what
    increase_batch_acc: 0.65  # batch size will be increased below this accuracy
    early_stop_auc: 0.8       # minimum auc for an early stop
    ft_dir: ['/app/embedding',
             '/home/ferreira/autodl_data/embedding',
             '/home/dingsda/data/embedding']  # paths to look for the embedding model

  data_manager:
    chi_word_length: 2
    max_valid_perclass_sample: 400
    max_sample_train: 18000
    max_train_perclass_sample: 800

  model_manager:
    embedding_dim: 300        # word embedding size





